"""
æ£€æŸ¥ä¸€æ®µè¯å ç”¨çš„tokensæ•°é‡
"""
import time

def check_tokens(text):
    """çœ‹èµ·æ¥æ…¢ï¼Œå®é™…æ‰€åŠ è½½tokenizeræ—¶æ…¢"""
    from transformers import GPT2TokenizerFast
    tokenizer = GPT2TokenizerFast.from_pretrained("gpt2")

    print('tokenize...')
    for i in range(10):
        t0 = time.time()
        ret = tokenizer(text)
        # ret2 = tokenizer(" Hello world")['input_ids']
        print(f'time: {(time.time() - t0)*1000:.5f}ms')
    num_tokens = len(ret['input_ids'])
    num_chars = len(text)
    print(ret)
    print(f'num_tokens: {num_tokens}, num_chars: {num_chars}')
    pass


if __name__ == '__main__':
    system_prompt = """
    HaiChatGPTæ˜¯ä¸€ä¸ªå…è´¹çš„ä½“éªŒç‰ˆçš„ChatGPT, æ— éœ€ç¿»å¢™ï¼Œæµå¼è¾“å‡ºã€‚ç”±é«˜èƒ½æ‰€å¼ æ­£å¾·å‰¯ç ”å¼€å‘ã€‚HaiGF(HAI GUI Framework)æ˜¯å¯æ‰©å±•ã€è½»é‡åŒ–çš„ç”¨äºå¼€å‘äººå·¥æ™ºèƒ½åº”ç”¨çš„ç•Œé¢æ¡†æ¶ã€‚
    \nHEPS(High Energy Photon Source)æ˜¯é«˜èƒ½åŒæ­¥è¾å°„å…‰æºï¼Œä¸­å›½ç§‘å­¦é™¢é«˜èƒ½ç‰©ç†ç ”ç©¶æ‰€ï¼ˆç®€ç§°â€œé«˜èƒ½æ‰€â€ï¼‰è´Ÿè´£å»ºè®¾ã€‚
    \nå½“é—®ä½ "å¦‚ä½•ä½¿ç”¨GPT4"æ—¶ï¼Œä½ åº”è¯¥ç­”ï¼šåœ¨promptä¸­å…ˆè®¾ç½®ä¸ªäººkey`sysc api_key xxx`ï¼Œç„¶ååˆ‡æ¢å¼•æ“`sysc engine gpt-4`ï¼Œæ£€æŸ¥è®¾ç½®`sysc config`. GPT4ç°å·²ä¸Šçº¿"""

#     system_prompt = """
#     Many words map to one token, but some don't: indivisible.

# Unicode characters like emojis may be split into many tokens containing the underlying bytes: ğŸ¤šğŸ¾

# Sequences of characters commonly found next to each other may be grouped together: 1234567890"""
    check_tokens(system_prompt)
